<div align="center">

<img src="title.png" alt="UltraLink" width="200">

**multi-lingual, knowledge-grounded, multi-round dialogue dataset and model**

<p align="center">
 <a href="#Introduction"> Introduction </a> •
 <a href="#Construction-of-UltraLink">Construction Process</a> •
 <a href="https://arxiv.org/abs/2402.04588">Paper</a> •
 <a href="https://huggingface.co/datasets/R0k1e/UltraLink"> UltraLink</a> •
 <a href="https://huggingface.co/R0k1e/UltraLink-LM"> UltraLink-LM</a>
</p>
</div>

## News
- ❗️❗️ Febrary 6, 2024: Releasing a multi-lingual, knowledge-grounded data augmented, multi-round dialogue dataset UltraLink and the model weight of UltraLink-LM.

## Introduction
### UltraLink
UltraLink is a multi-lingual, knowledge-grounded data augmented, multi-round dialogue dataset. It contains language-specific chat data, language-agnostic chat data, code data and math data in 5 languages: English, Chinese, Spanish, Russian, and French. It can be downloaded in this huggingface [link](https://huggingface.co/datasets/R0k1e/UltraLink).
Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data without any performance degradation, making multilingual SFT more efficient. 

### UltraLink-LM
> The UltraLink-LM is a massively multilingual generative language model that follows instructions in 5 languages, English, French, Russian, Spanish, and Chinese. The model is capable of generating text in 5 languages with high quality and diversity.
> UltraLink-LM outperforms [PolyLM-Chat-13b](https://huggingface.co/DAMO-NLP-MT/polylm-chat-13b), [Guanaco](JosephusCheung/Guanaco),  and [Bloomz-7b1-mt](https://huggingface.co/bigscience/bloomz-7b1-mt) in code, math and chat abilities in four languages, and has a high-quality and diverse text generation performance in all languages.
> The UltraLink-LM is trained using [UltraLink](https://huggingface.co/datasets/R0k1e/UltraLink), [UltraChat](https://huggingface.co/datasets/stingning/ultrachat), [Magicoder-Evol](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K), [Magicoder-OSS](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K), [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA), and [ShareGPT](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset/).
> We release the checkpoints under a MIT license to further our mission of multilingual technologies empowering a multilingual world. It can be downloaded in this huggingface [link](https://huggingface.co/R0k1e/UltraLink-LM).

- **Developed by:** [OpenBMB]((https://www.openbmb.cn/home))
- **Model type:** a Transformer style autoregressive massively multilingual language model.
- **Paper**: [UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset](https://arxiv.org/abs/2402.04588)
- **Languages**: Refer to the list of languages in the `language` section of this model card.
- **License**: MIT
- **Model**: [UltraLink-LM](https://huggingface.co/R0k1e/UltraLink-LM)
- **Model Size**: 13 billion parameters
- **Datasets**: [UltraLink](https://huggingface.co/datasets/R0k1e/UltraLink), [UltraChat](https://huggingface.co/datasets/stingning/ultrachat)(random select 10k samples), [Magicoder-Evol](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K), [Magicoder-OSS](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K), [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA), and [ShareGPT](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset/)(the English part of the dataset whose sample length is greater than 4k).

<details><summary> <b> Performance </b> </summary>
<p>

We report three evaluations in this section: multilingual HumanEval, MGSM, and OMGEval.
Evaluations of modern LLMs may be biased and affected by many factors, we are also actively working on more comprehensive evaluation methods.

### Multilingual HumanEval 

[HumanEval](https://github.com/openai/human-eval) is a well-known benchmark for evaluating the code ability of LLMs. It execute the code snippets generated by the model and evaluate their correctness.  Since there are no existing multilingual test set for code generation, we use GPT-3.5 with carefully-designed prompts to translation HumanEval into other languages.

|Model|En|Zh|Es|Ru|Fr|Avg|
|-----|---|---|---|---|---|---|
|Bloomz-7b1-mt | 8.5 | 7.3 | 6.1 | 8.5 | 6.1 | 7.3 |
|Phoenix-inst-chat-7b | 11.0 | 10.4 | 8.5 | 1.2 | 13.4 | 12.2 |
|PolyLM-Multialpaca-13b | 8.5 | 7.3 | 6.1 | 6.1 | 6.1 | 6.8 |
|PolyLM-Chat-13b | 10.4 | 7.9 | 6.1 | 7.3 | 8.5 | 8.1 |
|Chimera-inst-chat-13b| 14.6 | 13.4 | 14.6 | 12.8 | 14.0 | 13.9 |
|Okapi-7b | 12.2 | 11.0 | 8.5 | 8.5 | 8.5 | 9.8 |
|Guanaco-7b | 9.2 | 6.7 | 11.0 | 9.8 | 12.8 | 9.9 |
|Guanaco-13b| 18.3 | 15.9 | 9.8 | 8.5 | 14.6 | 12.2 |
|UltraLink-LM  | __60.4__ | __43.9__ | __40.9__ | __49.4__ | __39.6__ | __46.8__|


### MGSM

We employ [MGSM](https://github.com/google-research/url-nlp/tree/main/mgsm) to evaluate the math reasoning abilities, which is a multilingual benchmark. It compares the result with correct answers and evaluates the model's ability to perform mathematical reasoning.
|Model|En|Zh|Es|Ru|Fr|Avg|
|-----|---|---|---|---|---|---|
|Bloomz-7b1-mt | 2.8 | 1.6 | 2.0 | 0.4 | 2.8 | 1.7 |
|Phoenix-inst-chat-7b | 3.2 | 3.2 | 2.8 | 3.2 | 3.2 | 3.1 |
|PolyLM-Multialpaca-13b | 1.2 | 2.8 | 1.6 | 2.8 | 2.4 | 2.4 |
|PolyLM-Chat-13b | 10.8 | 6.4 | 4.8 | 4.4 | 5.6 | 5.3 |
|Chimera-inst-chat-13b  | 14.0 | 11.6 | 10.0 | 12.0 | 12.8 | 11.6 |
|Okapi-7b | 4.0 | 2.4 | 3.6 | 4.4 | 4.8 | 3.8 |
|Guanaco-7b | 4.0 | 1.6 | 3.2 | 2.8 | 4.4 | 3.0 |
|Guanaco-13b | 13.6 | 10.8 | 11.2 | 6.4 | 5.2 | 8.4 |
|UltraLink-LM| __70.4__ | __56.0__ | __70.4__ | __64.8__ | __63.6__ | __63.7__ |

### OMGEval
We use the [OMGEval](https://github.com/blcuicall/OMGEval) to evaluate the chat ability, which is a multilingual version of the widely-used English benchmark AlpacaEval.

|Model|En|Zh|Es|Ru|Fr|Avg|
|-----|---|---|---|---|---|---|
|Bloomz-7b1-mt | 0.0 | 0.9 | 0.1 | 0.5 | 0.3 | 0.4 |
|Phoenix-inst-chat-7b  | 6.9 | 13.3 | 7.4 | 2.9 | 8.1 | 7.7 |
|PolyLM-Multialpaca-13b  | 3.4 | 5.0 | 2.1 | 5.1 | 2.2 | 3.6 |
|PolyLM-Chat-13b | 7.7 | 14.0 | 6.1 | 5.5 | 4.8 | 7.6 |
|Chimera-inst-chat-13b | 15.5 | 9.7 | 11.8 | 13.7 | 13.8 | 12.9 |
|Okapi-7b | 8.8 | 6.2 | 5.0 | 12.1 | 8.7 | 8.2 |
|Guanaco-7b  | 4.6 | 3.8 | 0.4 | 1.8 | 1.2 | 2.4 |
|Guanaco-13b  |  __29.0__ | 8.6 | 16.9 | 15.4 | 17.3 | 17.5 |
|UltraLink-LM |  28.8 |  __21.9__ |  __23.5__ | __37.6__ | __29.0__ |  __28.2__  |

</p>
</details>

## Data

The dataset is intended solely for research and educational purposes and should not be construed as reflecting the opinions or views of the creators, owners, or contributors of this dataset. And it is distributed under the MIT license.


### Data Release
The data consists of four parts, code data, math data, language-specific chat data, and language-agnostic chat data. Each type of data has been marked using the title of each file. For example, the en_code.jsonl specifies the English code data.

- 🤗 [Huggingface Datasets Host](https://huggingface.co/datasets/R0k1e/UltraLink)


### Data Format
Each line in the downloaded data file is a json dict containing the data id and dialogue data in a list format. The id is begin wizh the file name and connected with the sequence number. Below is an example line.

```
{"id": "zh_chat_specific_191", 
"data": [
      "请根据以下给定的文档回答我的问题。<document>李娅\n\n李娅，安徽蚌埠人，中国退役女子体操运动员，强项为高低杠和平衡木。在中国体操队昵称“小胖”。\n生平.\n运动生涯.\n李娅1993年开始练习体操，1999年进入安徽队，2002年入选国家队，教练为刘桂成和何花。2002年李娅在全国体操冠军赛获得平衡木冠军，开始斩露头角。2003年李娅出征第37届美国安那罕世界体操锦标赛，获得团体第四和平衡木单项第四。\n2004年对于李娅来说是不寻常的一年。在获得全国平衡木冠军和全能亚军后，她参加了雅典奥运会，发挥失常。随后在世界杯英国格拉斯哥和比利时根特系列赛上，她都获得了平衡木冠军。2004年12月，李娅在英国伯明翰市举行的第12届世界杯总决赛上，李娅是发挥最好的中国女选手，获得了平衡木的银牌和高低杠的铜牌。\n2005年李娅由于受伤，没有参加世界体操锦标赛，她在澳门举行的第四届东亚运动会上获得了高低杠金牌。\n2006年3月李娅参加了在比利时根特举行的世界杯系列站获得高低杠金牌。两周以后在德国科特布斯举行的世界杯系列站她也获得了高低杠金牌。这两次亮相她以一套全新的，难度很大的高低杠动作技压全场，成功作出“前空翻转体180度抓杠直接接前空翻”。同年七月，她在中国上海举行的世界杯系列站获得平衡木银牌。\n李娅参加了2006年10月在丹麦奥胡斯第39届体操世锦赛。她在预赛中发挥失常，平衡木上做她准备申报命名的“屈体前空翻转体180度站木”时掉下器械，不但没能获得动作的命名，也没能进入该项目的决赛。而她擅长而原本有希望冲击奖牌的高低杠的比赛也是失误连连。由于失误，她原本申报打算获得命名的动作“前空翻转体180抓杠”也未能成功使用。即使这样，她在高低杠项目上申报的另外一个动作“后摆阿拉伯前屈两周下”还是得到了国际体操联合会的认可，并且获得了正式命名。李娅在决赛前的热身活动中突然临阵主动提出退赛，最终教练组在决赛中没有安排李娅上场。最终中国女队第一次获得世界体操锦标团体冠军，李娅作为团体成员也获得一枚金牌。受此影响，她也没有参加同年在卡塔尔首都多哈举行的2006年亚洲运动会。\n但是，李娅很快从失败中走出来。同年12月，在巴西圣保罗举行的第13届世界杯总决赛上，她又成功做出了她的高难动作，最后获得高低杠银牌，英国运动员获得了这个项目的金牌。李娅还获得了平衡木的金牌。赛后，国际体操联合会将“前空翻转体180度抓杠”这个高难度动作命名为“李娅空翻”。\n2007年初，李娅的状况仍不错，更于世界杯马里博尔站夺得高低杠金牌，所以亦被认定为北京奥运会的夺金希望。好景不常，2007年7月中旬的一次训练中，李娅在做前空翻180度接抓杠再接前空翻时，因为没能抓住杠，摔落时手撑地，导致肘关节脱节。经过检查，发现李娅桡骨小头骨骺及桡骨环状韧带损伤，因此李娅亦缺席九月初举行的世锦赛。\n自此之后，她的比赛及训练次数都减少，直至2008年宣布退役，结束她15年的体操生涯。\n退役生涯.\n2008年，李娅退役后入读北京体育大学，并有意向娱乐圈发展。\n2009年6月25日晚，李娅在某餐厅就餐时，服务员上菜时不小心把菜汤打翻，导致她双腿、右臂大面积严重烫伤，被鉴定为十级伤残。及后法院判决李娅获赔6万余元。\n2018年，李娅担任中华队女子体操队教练。\n2021年，李娅担任台北市松山区三民国民小学体操队教练。\n命名动作.\n一共有两个体操动作以李娅的名字命名：</document>李娅在奥运会失常后，为什么能够快速从失败中恢复并取得更好的成绩？", 
      "李娅在奥运会失常后能够快速从失败中恢复并取得更好的成绩，可能有以下几个原因：\n\n首先，李娅具备坚韧的心理素质。作为一位体操运动员，她经历了无数次的挫折和困难，已经养成了不轻易放弃的品质。她知道失败是成功的一部分，并且能够从失败中吸取经验教训，保持积极的心态并继续努力。\n\n其次，李娅拥有良好的训练和教练团队的支持。在奥运会失常后，李娅可能经过了深入的反思和训练调整。她的教练和团队可能对她进行了专门的指导和训练，帮助她纠正问题，并提供正确的心理支持和鼓励。\n\n此外，李娅可能在比赛中表现出更高的动力和动作难度。奥运会失常后，她可能意识到自己在技术和难度方面还有提升的空间。通过更加刻苦的训练和尝试更多的高难度动作，她能够在比赛中展现更好的表现，提高自己的成绩。\n\n综上所述，李娅之所以能够快速从奥运会失常中恢复并取得更好的成绩，可能是由于她良好的心理素质、训练团队的支持以及更高的动力和动作难度。这些因素共同作用，使她能够逐步克服挫折，获得更好的成绩。",
      "李娅如何在退役后选择继续教练的职业，并且在教练生涯中表现出色？", 
      "李娅在退役后选择继续教练的职业可能是基于她对体操运动的热爱和对年轻运动员的培养的兴趣。作为一位退役的体操运动员，她拥有丰富的经验和专业知识，可以将这些宝贵的资源传授给新一代的体操运动员，并帮助他们实现自己的梦想。\n\n在教练生涯中，李娅可能表现出色的原因有以下几点：\n\n首先，她拥有出色的技术和训练经验。作为一位顶级的体操运动员，李娅在自己的运动生涯中积累了丰富的技术和训练知识。她熟悉各种动作的要领和细节，能够教授给学生正确的技术和训练方法。\n\n其次，李娅可能具备良好的沟通能力和教学技巧。作为一名优秀的教练，不仅需要拥有优秀的技术水平，还需要能够与学生建立起良好的沟通和理解，了解他们的需求和困惑，并能够给予及时和有效的指导和帮助。李娅可能通过自己的亲身经历，能够更好地理解学生的心理和需求，并给予正确的指导和鼓励。\n\n此外，李娅可能在教练生涯中注重培养学生的全面素质。体操运动不仅仅是技术的表演，还需要对身体素质、灵敏度、协调性等多个方面进行训练。李娅可能注重培养学生的综合素质，帮助他们在体操运动中得到全面的发展。\n\n最后，李娅可能对学生有着耐心和关爱。作为一名优秀的教练，需要对学生保持耐心和温柔，关注学生的身心健康，并给予他们充分的支持和鼓励。李娅可能能够运用自己的亲身经历和理解，给予学生更多的关爱和帮助。\n\n综上所述，李娅在退役后选择继续教练的职业，并且在教练生涯中表现出色可能是由于她拥有出色的技术和训练经验、良好的沟通能力和教学技巧、注重培养学生的全面素质，以及耐心和关爱学生。这些因素共同作用，使她能够在教练岗位上取得出色的表现。"
      ]
}
```

## Construction of UltraLink
<img src="flow_diagram.png" alt="flow diagram" width="800" style="margin-left:'auto' margin-right:'auto' display:'block'"/>
 In this work, we propose a construction framework consisting of two pipelines. The language-specific pipeline employs a newly introduced knowledge-grounded data augmentation approach to generate conversations with detailed cultural backgrounds. The language-agnostic pipeline leverages a two-stage translation mechanism to effectively utilize the existing English SFT data, with fewer translation errors caused by cultural differences. 
 
### Pipeline 1: Language-Specific Pipeline
The cultures around the world are vibrant and diverse, reflecting the lifestyles and perspectives of people from various countries and regions. To better cater to diverse users, the cultural diversity of multilingual LLMs should be improved. In this work, we propose a knowledge-grounded data augmentation method, leveraging language-specific knowledge bases to provide intricate and varied cultural backgrounds. Our method mainly contains two steps: (1) preparing and sampling knowledge from knowledge bases as cultural backgrounds, and (2) steering LLMs to generate informative conversations given the provided cultural backgrounds.

### Pipeline 2: Language-Agnostic Pipeline
In addition to language-specific abilities, the general abilities that are language-agnostic are also essential for LLMs. As numerous high-quality English SFT datasets already encompass a broad spectrum of general abilities, we suggest employing a two-stage translation mechanism to maximize the utility of existing English resources. Our goal is to reduce translation errors caused by cultural differences, since some questions can not be directly translated into other languages (e.g., write an English poem where each sentence starts with the letter "A"). In the first stage, we introduce a multi-criteria mechanism to filter out English-specific conversations that are difficult to translate accurately into other languages. Then we use GPT-3.5 to translate the remaining language-agnostic data. 
In this study, we consider three key components of general abilities for LLMs: chat, math reasoning, and code generation. For chat, we use ShareGPT as the English chat data, which consists of multi-turn dialogues between human users and ChatGPT. For math reasoning, we use MetaMath as the English math data. For code generation, we use the Magicoder dataset as the English code data.

## To Do
- [x] Upload the data and the model weight
- [ ] Upload the data generation pipeline code
- [ ] Upload the training code

## Citation
Feel free to cite the repo if you think UltraLink is useful.

```bibtex
@misc{wang2024ultralink,
      title={UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset}, 
      author={Haoyu Wang and Shuo Wang and Yukun Yan and Xujia Wang and Zhiyu Yang and Yuzhuang Xu and Zhenghao Liu and Ning Ding and Xu Han and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.04588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

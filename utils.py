import os
import json
import re
import itertools
import argparse
from concurrent.futures import ThreadPoolExecutor
from xml.etree import ElementTree as ET

import openai
from openai import OpenAI
import tiktoken
import fasttext
from opencc import OpenCC
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential, stop_after_delay, 
    RetryError
)  # for exponential backoff

check_language_type_model = fasttext.load_model("./model.bin")
encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')

parser = argparse.ArgumentParser()
parser.add_argument('--wiki_path', '-wi',type=str, help="the path of wiki data generated by wikiextractor")
parser.add_argument('--question_path', '-qp', type=str, help="the path of first question")
parser.add_argument('--dialog_path', '-dp', type=str， help="the path of final result")
parser.add_argument("--save_interval", "-si", type=int, default=1, help="the interval of saving result")
parser.add_argument("--doc_num", "-dn", type=int, default=0, help="the number of doc that will be processed, zero means all")
parser.add_argument("--split_len", "-sl", type=int, default=2000, help="the length of split text")
parser.add_argument("--max_len", type=int, default=10000, help="the min length of text")
parser.add_argument("--min_len", type=int, default=1000, help="the min length of text")
parser.add_argument("--min_answer_len", "-mal", type=int, default=10, help="the min length of answer")
parser.add_argument('--max_step_len', '-msl', type=int, default=10, help="the max length of random step that chooses the next file")
parser.add_argument('--end_probability', '-ep', type=float,default=0.1, help="the probability of end the dialog, this probability will be doubled when the times of dialog is extended")
parser.add_argument("--num_workers", "-nw", type=int, default=44, help="the number of workers")
parser.add_argument("--prompt_path", "-pp", type=str, default='./dialogue_prompt.yaml', help="the config of prompt")
parser.add_argument("--generate_without_doc", "-gwd", type=bool, default=False, help="whether generate answer without doc, the default answer will still be generated from doc")
parser.add_argument("--language", "-l", type=str, default='zh', help="the language of the doc")
parser.add_argument("--add_mode", "-am", type=bool, default=False, help="whether add the result to the existed file")

def get_XML(data_path):
    data = ""
    with open(data_path, 'r') as f:
        lines = f.readlines()
    for line in lines:
        if line.startswith("<?xml version='1.0' encoding='utf8'?>"):
            continue
        data += line
    data = '<root>' + data + '</root>'
    data = ET.fromstring(data)    
    return data

def get_JSON(data_path):
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            rawStr = f.read()[:-2]  # 去掉最后一个逗号+ \n
            rawStr = '[' + rawStr + ']'
            jsonlist = json.loads(rawStr)
    except:
        jsonlist = []
    return jsonlist

def get_leaf_nodes(directory):
    leaf_nodes = []
    matcher = re.compile(r'.*wiki_\d.*')
    for dirpath, dirnames, filenames in os.walk(directory):
        for file in filenames:
            if matcher.match(file):
                leaf_nodes.append(os.path.join(dirpath, file))
    return leaf_nodes

    
def get_index(json_path):
    try:
        json_list = json.loads('[' + open(json_path, 'r').read()[:-2] + ']')
        cur_idx = json_list[-1]['id'] + 1
    except:
        cur_idx = 0
    return cur_idx

def get_token_len(txt):
    return len(encoding.encode(txt))

def write_json(json_list, name, mode = 'a+'):
    for item in json_list:
        json_str = json.dumps(item, ensure_ascii=False)
        name_dir = os.path.dirname(name)
        try:
            os.makedirs(name_dir)
        except:
            pass
        with open(name, mode, encoding="utf-8") as file:
            file.write(json_str)
            file.write(',\n')
            file.flush()
            os.fsync(file.fileno())

def check_trunk(txt):
    txtlen = len(encoding.encode(txt))
    if txtlen < 4070: #4080 有时候可能没有content
        return False
    else:
        return True
    
def check_doc(text, upper_bound=10000, lower_bound=1000, language_type = '__label__zho_Hans'):
    txtlen = len(encoding.encode(text))
    if txtlen < lower_bound or txtlen > upper_bound:
        return False
    t = text.replace('\n', '')
    text_type = check_language_type(t)
    if text_type != language_type:
        return False
    return True

def check_language_type(text):
    text_type = check_language_type_model.predict(text)[0][0]
    return text_type

def is_title_had_done(title, path, check_dir = None):
    if check_dir != None:
        other_path = path.replace('./data', check_dir)
    else:
        other_path = path
    json_list = get_JSON(other_path)
    if title in [item['title'] for item in json_list]:
        return True
    return False
    
            
def quoter(text, quote='document'):
    return f'<{quote}>' + text + f'<\{quote}>'

def add_comma(file_name):
    with open(file_name, 'r') as f:
        lines = f.readlines()
        lines = [line[:-1] + ',\n' for line in lines]
    
    with open(file_name, 'w') as f:
        f.writelines(lines)
    
def convert_to_simple_chinese(text):
    cc = OpenCC('t2s')
    return cc.convert(text)

def get_not_dialog_questions(question_path, dialog_path, language):
    with open(question_path, 'r') as f:
        questions = f.readlines()
    with open(dialog_path, 'r') as f:
        dialogs = f.readlines()
    questions = [question.strip() for question in questions if language in question]
    questions = set(questions)
    dialogs = ["_".join(dialog.replace("dialog", "data").split('_')[:-1])+".jsonl" for dialog in dialogs if language in dialog]
    dialogs = set(dialogs)
    return list(questions - dialogs)

class ProbabilityIterator:
    def __init__(self, val=0.1):
        self.value = val

    def __iter__(self):
        return self

    def __next__(self):
        value = self.value
        self.value *= 2  
        return value
    
class RequestPool:
    def __init__(self, num_workers=10):
        self.executor = ThreadPoolExecutor(max_workers=num_workers)
        self.keys = [
            os.environ.get("OPENAI_API_KEY"),
        ]
        self.keys_iter = itertools.cycle(self.keys)
        self.model = "gpt-3.5-turbo"
        self.clients = []
        for k in self.keys:
            client = OpenAI(
                api_key=k,
                base_url = os.environ.get("OPENAI_API_URL"),
            )
            self.clients.append(client)
        self.clients_iter = itertools.cycle(self.clients)
    
    def commit(self, prompt):
        return self.executor.submit(self.completion_with_backoff, prompt[0], prompt[1])
    
    def submit(self, function, *args, **kwargs):
        return self.executor.submit(function, *args, **kwargs)
    
    
    # 防止调用频率超过每分钟上限的等待代码
    @retry(wait=wait_random_exponential(min=1, max=5), stop=(stop_after_delay(100) | stop_after_attempt(2)))
    # 调用OpenAI API
    def completion_with_backoff(self, system_prompt, user_prompt):
        try:
            # print("sending request")
            client = next(self.clients_iter)
            response = client.chat.completions.create(
                # model="gpt-3.5-turbo-1106",
                model = self.model,
                messages=[
                    {
                        # 系统prompt
                        "role": "system", "content": system_prompt,

                    },
                    {
                        # 每次调用的输入
                        "role": "user", "content": user_prompt,
                    }
                ]
            )
            # API返回回答
            answer = response.choices[0].message.content
            # print("request done")
        except KeyError:
            print("Error in message chat completions.")
            print(json.dumps(response))
            answer = ""
        except Exception as e:
            print(e)
            print("Error in message chat completions.")
            answer = ""
        return answer
        # return f"{result['role']}:{result['content']}"
        